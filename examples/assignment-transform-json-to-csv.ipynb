{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bec8f5c-90cb-4936-a161-b1f1c53ed2df",
   "metadata": {},
   "source": [
    "# Extracting and Transforming Metadata\n",
    "\n",
    "This notebook illustrates a second step in the \"transform\" process.\n",
    "Although there are many possible data transformation pathways, this demonstration\n",
    "illustrates a process that converts metadata into an open, non-proprietary,\n",
    "text-based format: the CSV file.\n",
    "\n",
    "As throughout, the general process here follows the generalized \"Extract - Transform - Load\" process, which is frequently the abstract model for pulling data from one system, transporting, cleaning, and outputting to another system, which remains the overall workflow.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "After completing the assignment associated with this notebook, you should: \n",
    "\n",
    "* Use programming (Python) to work with data supplied by an API in JSON format to manage and transform useful parts of that data into a CSV format.\n",
    "* Create ingest-ready collection metadata that conforms to Dublin Core and other digital collection metadata standards, which can be used to load content into another site (in this case, a new collection platform, like Omeka S or CollectionBuilder).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The main steps outlined in this notebook are as follows:\n",
    "\n",
    "* **Transform the metadata.** This step assumes you have already developed a plan for transforming your data, which is your Metadata Application Profile, or MAP.\n",
    "  1. Develop your transformation script with a small subset of the metadata. In this case, one record.\n",
    "  1. Transform the data you've gathered in JSON into a CSV file according to the metadata crosswalk you've developed. The goal in this step is to create a CSV that we can use to import items into your Omeka site (using the CSV Import module). Note that the code outlined here suggests how all of these data elements may be extracted and transformed, but it does not necessarily output all of the elements that you will need to complete your assignment. In other words, there is still work to do to complete this code, but you are welcome to adopt or reuse the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a281bf-4fb1-4bb2-b01f-d4f1c523e905",
   "metadata": {},
   "source": [
    "# Get collection list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81df05-072c-47cd-b4dc-b3abdbaa8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# for working with local files\n",
    "import glob\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45a3bb-b954-44d9-9446-97a049c9b974",
   "metadata": {},
   "source": [
    "# Transformation Part 1: Testing\n",
    "\n",
    "At a high level, the transformation step involves the creation of code or another implementation workflow, which will search the item metadata files downloaded previously, extract the target fields identified in the MAP, then write that information to a CSV for ingest or loading into a new presentation platform.\n",
    "\n",
    "First, develop a search pattern for identifying the desired JSON files. Here, you create a list of the files that you want to transform, called `list_of_item_metadata_files`. \n",
    "\n",
    "**Reminder:** This step builds on your terminal skills! (And builds on your understanding of regular expressions and shell navigation. Note, however, these are technically file path expansions, not actual regular expressions. The general idea of creating a pattern and asking the computer to respond with a list of results that meet your criteria, is similar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899bee6-bed9-4240-85ed-1c946110f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_loc = os.getcwd()\n",
    "\n",
    "print(current_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e195bef-fe55-47e7-a1af-5ecf0c0f324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_path = os.path.join('collection-site-materials','item-metadata')\n",
    "\n",
    "print(metadata_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6363a",
   "metadata": {},
   "source": [
    "The next cell uses the `glob` library, which supports the use of file path expanders\n",
    "to look for patterns in file paths. In this case, the previous item metadata exraction\n",
    "wrote files that had the pattern `item_metadata-[item-identifier].json`. \n",
    "So, to match any pattern for the `item-identifier` section, `glob` allows\n",
    "the use of the `*` (asterisk) character to match any pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4a730-4208-4e63-8531-4b6da696ec0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_count = 0\n",
    "\n",
    "for file in glob.glob('../collection-site-materials/item-metadata/item_metadata-*.json'):\n",
    "    file_count += 1\n",
    "    print(file)\n",
    "    \n",
    "print('found',file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8178914-945e-4bc9-a890-b717db15af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_item_metadata_files = list() \n",
    "for file in glob.glob('../collection-site-materials/item-metadata/item_metadata-*.json'):\n",
    "    list_of_item_metadata_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded9141-1613-48db-a74c-ddafd86eb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_item_metadata_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb38fc-23d5-4b1d-8250-478f21deff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick duplicate check\n",
    "list_of_item_metadata_files.sort()\n",
    "\n",
    "for file in list_of_item_metadata_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f634f77",
   "metadata": {},
   "source": [
    "To develop your data transformation and metadata profile, \n",
    "first you need to explore the information that you have about each item. \n",
    "To do this, explore one item to understand how the information is structured.\n",
    "How do you open the json? How is it structured? Where is the information you want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96909e-2cce-4ca6-95b9-7d1c03c74c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try first with one file, can you open the json, can you see what elements are in the json?\n",
    "with open(list_of_item_metadata_files[0], 'r', encoding='utf-8') as item:\n",
    "    # what are we looking at?\n",
    "    print('file:',list_of_item_metadata_files[0],'\\n')\n",
    "    \n",
    "    # load the item data\n",
    "    item_data = json.load(item)\n",
    "    \n",
    "    for element in item_data.keys():\n",
    "        print(element,':',item_data[element])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1b077",
   "metadata": {},
   "source": [
    "Look around in the dictionary a bit more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49f8c4",
   "metadata": {},
   "source": [
    "For the development of your metadata transformation, you're looking for \n",
    "how to extract the elements identified in the MAP table. For example, which date fields do you want and where are they? Where will you find the format information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # can you get the date?\n",
    "    print('\\ndate:',item_data['date'], type(item_data['date']))\n",
    "    # can you get the format?\n",
    "    print('\\nformat:',item_data['format'][0], type(item_data['format']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be2cad-c5ef-4184-a2ea-3ae2da53db0f",
   "metadata": {},
   "source": [
    "## Test: Try it with one example\n",
    "\n",
    "First, try to set up the extract process with one example. This may get more complicated later since you don't know yet if every item has the same metadata attributes in the JSON. But start with some basics and build up from there. \n",
    "\n",
    "For a first pass, look out for these items, and find where in the JSON you can locate them:\n",
    "\n",
    "* 'item_id'\n",
    "* 'title'\n",
    "* 'date' \n",
    "* 'source_url'\n",
    "* 'phys_format'\n",
    "* 'dig_format'\n",
    "* 'rights'\n",
    "\n",
    "_Hint: use the JSON viewer in JupyterLab, use an extension in VSCode, or use a browser to look through sample JSON. The block below uses item `cph.3b41963`._\n",
    "\n",
    "You may need to use try/except patterns to create workarounds for cases where some items may not have exactly the same attributes that you've identified in your test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cbb86-d872-4e38-8673-0e4e6e2c06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the containers to create the csv of all the item fields\n",
    "# file for csv to read out\n",
    "collection_info_csv = 'collection_items_data.csv'\n",
    "\n",
    "# set up a list for the columns in your csv; \n",
    "# your goal should be to automate this, but . . . \n",
    "# it works for demonstration as you set up the crosswalk\n",
    "headers = ['source_file', 'item_id', 'title', 'date', 'source_url', 'phys_format', 'dig_format', 'rights']\n",
    "\n",
    "# try first with one file\n",
    "with open(list_of_item_metadata_files[0], 'r', encoding='utf-8') as data:\n",
    "    # load the item data\n",
    "    item_data = json.load(data)\n",
    "    \n",
    "    # extract the data you want\n",
    "    # for checking purposes, add in the source of the info\n",
    "    source_file = str(file)\n",
    "    # make sure there's some unique and stable identifier\n",
    "    try:\n",
    "        item_id = item_data['library_of_congress_control_number']\n",
    "    except:\n",
    "        item_id = item_data['url'].split('/')[-2]\n",
    "    title = item_data['title']\n",
    "    date = item_data['date']\n",
    "    source_url = item_data['url']\n",
    "    try:\n",
    "        phys_format = item_data['format'][0]\n",
    "    except:\n",
    "        phys_format = 'Not found'\n",
    "    try:\n",
    "        dig_format = item_data['online_format'][0]\n",
    "    except:\n",
    "        dig_format = 'Not found'\n",
    "    mime_type = item_data['mime_type']\n",
    "    try:\n",
    "        rights = item_data['rights_information']\n",
    "    except:\n",
    "        rights = 'Undetermined'\n",
    "\n",
    "\n",
    "    # dictionary for the rows\n",
    "    row_dict = dict()\n",
    "    \n",
    "    # look for the item metadata, assign it to the dictionary; \n",
    "    # start with some basic elements likely (already enumerated in the headers list) :\n",
    "    # source file\n",
    "    row_dict['source_file'] = source_file\n",
    "    # identifier\n",
    "    row_dict['item_id'] = item_id\n",
    "    # title\n",
    "    row_dict['title'] = title\n",
    "    # date\n",
    "    row_dict['date'] = date\n",
    "    # link\n",
    "    row_dict['source_url'] = source_url\n",
    "    # format\n",
    "    row_dict['phys_format'] = phys_format\n",
    "    # digital format\n",
    "    row_dict['dig_format'] = dig_format\n",
    "    #rights\n",
    "    row_dict['rights'] = rights \n",
    "    print('created row dictionary:',row_dict)\n",
    "\n",
    "    # write to the csv\n",
    "    with open(collection_info_csv, 'w', encoding='utf-8') as fout:\n",
    "        writer = csv.DictWriter(fout, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "        print('wrote',collection_info_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1d9c5-4ec4-45e2-b9a0-38ebdd6cb812",
   "metadata": {},
   "source": [
    "You're now developing the structure of the CSV file that will import items into your Omeka S site. The CSV import module supports the loading of item files via a URL. This provides the location of a file (in this case, an image), which Omeka will copy into its database and attach to your item. This means that it isn't necessary to upload individual files after or during metadata creation. \n",
    "\n",
    "To allow this, you need to find a direct url to a good image file for the item. There are multiple options, and the code below demonstrates looking for the url to a medium-sized image of an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b57056-6708-488f-a196-3a69f477cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info_csv = 'collection_items_data.csv'\n",
    "\n",
    "# set up a list for the columns in your csv; in future, this should be more automated but this works for now as you set up the crosswalk\n",
    "headers = ['source_file', 'item_id', 'title', 'date', 'source_url', 'phys_format', 'dig_format', 'rights']\n",
    "\n",
    "# try first with one file\n",
    "with open(list_of_item_metadata_files[0], 'r', encoding='utf-8') as data:\n",
    "    # load the item data\n",
    "    item_data = json.load(data)\n",
    "    \n",
    "    print(item_data['image_url'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec775d7-ab10-45c3-88ad-40706c100967",
   "metadata": {},
   "source": [
    "# Transformation Part 2: Write your CSV\n",
    "\n",
    "The goal of this final step is to create a CSV file, which will be possible to import into your Omeka site. It may seem like it's taken a long time to get to this point... but remember, when this works you will be importing around 60 items into the site at one time, so if you can get all of this to work for an even larger set of materials, you will be saving quite a lot of time in the future when you need to import items. Even if you were to collect the items piecemeal, which would need a different workflow than illustrated here, you can accomplish similar goals by recording metadata for each item consistently and in a spreadsheet, which you can then use to import the items in batch.\n",
    "\n",
    "So now that your transformation script is tested, the goal is to extend this to the whole set by looping through each of the desired JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279cbef-6e98-4537-a013-5feef216d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for purposes of demonstration, use this block to make sure there isn't already a list file:\n",
    "\n",
    "items_data_file = os.path.join('collection_items_data.csv')\n",
    "\n",
    "if os.path.isfile(items_data_file):\n",
    "    os.unlink(items_data_file)\n",
    "    print('removed',items_data_file)\n",
    "\n",
    "# clear row_dict\n",
    "row_dict = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd445a1a-473f-432b-942b-3cd40fd3cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "date_string_for_today = date.today().strftime('%Y-%m-%d') # see https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n",
    "\n",
    "print(date_string_for_today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6546cb-d74c-46b7-8286-80df234878a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the containers to create the csv & counters \n",
    "# file for csv to read out\n",
    "collection_info_csv = os.path.join('..','collection-site-materials','collection_items_data.csv')\n",
    "file_count = 0\n",
    "items_written = 0\n",
    "error_count = 0\n",
    "\n",
    "# add in a couple of extras for Omeka, including item type and date uploaded\n",
    "\n",
    "# set up a list for the columns in your csv; in future, this should be more automated but this works for now as you set up the crosswalk\n",
    "headers = ['item_type', 'date_uploaded', 'source_file', 'item_id', 'title', 'date', 'source_url', 'phys_format', 'dig_format', 'rights', 'image_url']\n",
    "\n",
    "# now, adapt the previous loop to open each file:\n",
    "for file in list_of_item_metadata_files:\n",
    "    file_count += 1\n",
    "    print('opening',file)\n",
    "    with open(file, 'r', encoding='utf-8') as item:\n",
    "        # load the item data\n",
    "        try:\n",
    "            item_data = json.load(item)\n",
    "        except:\n",
    "            print('error loading',file)\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "        # extract/name the data you want\n",
    "        # item type\n",
    "        item_type = 'Item'\n",
    "        # date uplaoded\n",
    "        date_uploaded = date_string_for_today\n",
    "        # for checking purposes, add in the source of the info\n",
    "        source_file = str(file)\n",
    "        # make sure there's some unique and stable identifier\n",
    "        try:\n",
    "            item_id = item_data['library_of_congress_control_number']\n",
    "        except:\n",
    "            item_id = item_data['url'].split('/')[-2]\n",
    "        title = item_data['title']\n",
    "        try:\n",
    "            date = item_data['date']\n",
    "        except:\n",
    "            date = 'Not found'\n",
    "        source_url = item_data['url']\n",
    "        try:\n",
    "            phys_format = item_data['format'][0]\n",
    "        except:\n",
    "            phys_format = 'Not found'\n",
    "        try:\n",
    "            dig_format = item_data['online_format'][0]\n",
    "        except:\n",
    "            dig_format = 'Not found'\n",
    "        mime_type = item_data['mime_type']\n",
    "        try:\n",
    "            rights = item_data['rights_information']\n",
    "        except:\n",
    "            rights = 'Undetermined'\n",
    "        try:\n",
    "            image_url = item_data['image_url'][3]\n",
    "        except:\n",
    "            image_url = 'Did not identify a URL.'\n",
    "\n",
    "        # dictionary for the rows\n",
    "        row_dict = dict()\n",
    "\n",
    "        # look for the item metadata, assign it to the dictionary; \n",
    "        # start with some basic elements likely (already enumerated in the headers list) :\n",
    "        # item type\n",
    "        row_dict['item_type'] = item_type\n",
    "        # date uploaded\n",
    "        row_dict['date_uploaded'] = date_uploaded\n",
    "        # source filename\n",
    "        row_dict['source_file'] = source_file\n",
    "        # identifier\n",
    "        row_dict['item_id'] = item_id\n",
    "        # title\n",
    "        row_dict['title'] = title\n",
    "        # date\n",
    "        row_dict['date'] = date\n",
    "        # link\n",
    "        row_dict['source_url'] = source_url\n",
    "        # format\n",
    "        row_dict['phys_format'] = phys_format\n",
    "        # digital format\n",
    "        row_dict['dig_format'] = dig_format.capitalize()\n",
    "        #rights\n",
    "        row_dict['rights'] = rights\n",
    "        #image\n",
    "        row_dict['image_url'] = image_url\n",
    "\n",
    "        # write to the csv\n",
    "        with open(collection_info_csv, 'a', encoding='utf-8') as fout:\n",
    "            writer = csv.DictWriter(fout, fieldnames=headers)\n",
    "            if items_written == 0:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(row_dict)\n",
    "            items_written += 1\n",
    "            print('adding',item_id)\n",
    "\n",
    "print('\\n\\n--- LOG ---')\n",
    "print('wrote',collection_info_csv)\n",
    "print('with',items_written,'items')\n",
    "print(error_count,'errors (info not written)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61260c5d-0bfd-4dc4-8c1f-e113b75b9fce",
   "metadata": {},
   "source": [
    "Now, you should have a well-formed, complete CSV file at `data/collection_items_data.csv`. This file should ahve all the information to import the 59 items that you were able to identify. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
